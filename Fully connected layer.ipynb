{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eac477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "file = 'NC'    #  You can modify the training data set here, for example: BBB, NC, NA, NT\n",
    "\n",
    "file_prefixes = [\"smiles\",\"DMPNN\", \"ecfp\", \"mfbert\", \"padel\"]\n",
    "file_extension = \".csv\"\n",
    "\n",
    "file_prefix = f\"{file}_Feature_fusion/data_train\"\n",
    "file_extension = \".csv\"\n",
    "\n",
    "val_file_prefix = f\"{file}_Feature_fusion/data_valid\"\n",
    "val_file_extension = \".csv\"\n",
    "\n",
    "start_index = 0\n",
    "end_index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acae8222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [00:10<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(start_index, end_index + 1), desc=\"Processing files\"):  # 使用 tqdm 添加进度条  \n",
    "    combined_data_train = None\n",
    "    combined_data_valid = None\n",
    "    combined_data_test = None\n",
    "    for file_feature in file_prefixes:\n",
    "        #从randomi/中获取train，valid，test    \n",
    "        \n",
    "        file_path_train = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"train\" + file_extension\n",
    "        data_train = pd.read_csv(file_path_train) \n",
    "                \n",
    "        file_path_valid = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"valid\" + file_extension\n",
    "        data_valid = pd.read_csv(file_path_valid) \n",
    "\n",
    "        file_path_test = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"test\" + file_extension\n",
    "        data_test = pd.read_csv(file_path_test)   \n",
    "        \n",
    "        data_train = pd.read_csv(file_path_train)     \n",
    "        data_valid = pd.read_csv(file_path_valid)   \n",
    "        data_test  = pd.read_csv(file_path_test) \n",
    "        \n",
    "        if combined_data_train is None:\n",
    "            combined_data_train = data_train\n",
    "        if combined_data_valid is None:\n",
    "            combined_data_valid = data_valid\n",
    "        if combined_data_test is None:   \n",
    "            combined_data_test  = data_test\n",
    "        else:\n",
    "            combined_data_train = pd.concat([combined_data_train, data_train], axis=1)     \n",
    "            combined_data_valid = pd.concat([combined_data_valid, data_valid], axis=1) \n",
    "            combined_data_test = pd.concat([combined_data_test, data_test], axis=1) \n",
    "    \n",
    "    folder_path = f'{file}_Feature_fusion'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    combined_data_train.to_csv(f'{file}_Feature_fusion/data_train{i}.csv', index=False)\n",
    "    combined_data_valid.to_csv(f'{file}_Feature_fusion/data_valid{i}.csv', index=False)\n",
    "    combined_data_test.to_csv(f'{file}_Feature_fusion/data_test{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802b00cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smiles    Cc1cc(c(cc1)C)[N+](=O)[O-]\n",
      "active                             0\n",
      "0                           0.098735\n",
      "1                                0.0\n",
      "2                                0.0\n",
      "                     ...            \n",
      "676                         0.179503\n",
      "677                         0.078543\n",
      "678                              0.0\n",
      "679                              0.0\n",
      "680                         0.384523\n",
      "Name: 0, Length: 2775, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2502, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2502429485321045, Val Loss: 0.2433009296655655\n",
      "model_0训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2433, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.2433009296655655, Val Loss: 0.23311583697795868\n",
      "model_0训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2331, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.23311583697795868, Val Loss: 0.22039970755577087\n",
      "model_0训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2204, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.22039970755577087, Val Loss: 0.20251068472862244\n",
      "model_0训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2025, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.20251068472862244, Val Loss: 0.18168707191944122\n",
      "model_0训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.1817, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.18168707191944122, Val Loss: 0.15664294362068176\n",
      "model_0训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1566, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.15664294362068176, Val Loss: 0.1316172480583191\n",
      "model_0训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1316, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.1316172480583191, Val Loss: 0.10991762578487396\n",
      "model_0训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1099, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.10991762578487396, Val Loss: 0.09078644961118698\n",
      "model_0训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.0908, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.09078644961118698, Val Loss: 0.0756344422698021\n",
      "model_0训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.0756, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.0756344422698021, Val Loss: 0.06508839875459671\n",
      "model_0训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.0651, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.06508839875459671, Val Loss: 0.05830442160367966\n",
      "model_0训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.0583, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.05830442160367966, Val Loss: 0.05381350964307785\n",
      "model_0训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.0538, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.05381350964307785, Val Loss: 0.05049917846918106\n",
      "model_0训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.0505, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.05049917846918106, Val Loss: 0.04765567183494568\n",
      "model_0训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.0477, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.04765567183494568, Val Loss: 0.044924795627593994\n",
      "model_0训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0449, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.044924795627593994, Val Loss: 0.042175404727458954\n",
      "model_0训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0422, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.042175404727458954, Val Loss: 0.03937022387981415\n",
      "model_0训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0394, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.03937022387981415, Val Loss: 0.036521293222904205\n",
      "model_0训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0365, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.036521293222904205, Val Loss: 0.033655911684036255\n",
      "model_0训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0337, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.033655911684036255, Val Loss: 0.03083924949169159\n",
      "model_0训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0308, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.03083924949169159, Val Loss: 0.028201747685670853\n",
      "model_0训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0282, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.028201747685670853, Val Loss: 0.02588660642504692\n",
      "model_0训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0259, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.02588660642504692, Val Loss: 0.0245906263589859\n",
      "model_0训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0246, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.0245906263589859, Val Loss: 0.026960404589772224\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0270, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.026960404589772224, Val Loss: 0.026355724781751633\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0264, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.026355724781751633, Val Loss: 0.020802689716219902\n",
      "model_0训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0208, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.020802689716219902, Val Loss: 0.02040410414338112\n",
      "model_0训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0204, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.02040410414338112, Val Loss: 0.02167377807199955\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0217, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.02167377807199955, Val Loss: 0.017307061702013016\n",
      "model_0训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0173, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.017307061702013016, Val Loss: 0.01800156570971012\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0180, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.01800156570971012, Val Loss: 0.01750502921640873\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0175, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.01750502921640873, Val Loss: 0.01459586713463068\n",
      "model_0训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.01459586713463068, Val Loss: 0.015654226765036583\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0157, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.015654226765036583, Val Loss: 0.0139781404286623\n",
      "model_0训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.0139781404286623, Val Loss: 0.01283105555921793\n",
      "model_0训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0128, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.01283105555921793, Val Loss: 0.013396335765719414\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.013396335765719414, Val Loss: 0.011640294454991817\n",
      "model_0训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.011640294454991817, Val Loss: 0.011846307665109634\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0118, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.011846307665109634, Val Loss: 0.011642610654234886\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.011642610654234886, Val Loss: 0.010394388809800148\n",
      "model_0训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.010394388809800148, Val Loss: 0.011060901917517185\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.011060901917517185, Val Loss: 0.010221891105175018\n",
      "model_0训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.010221891105175018, Val Loss: 0.009627021849155426\n",
      "model_0训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.009627021849155426, Val Loss: 0.010090873576700687\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.010090873576700687, Val Loss: 0.00909219030290842\n",
      "model_0训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.00909219030290842, Val Loss: 0.008936685509979725\n",
      "model_0训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.008936685509979725, Val Loss: 0.009112633764743805\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.009112633764743805, Val Loss: 0.008281867019832134\n",
      "model_0训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.008281867019832134, Val Loss: 0.008274821564555168\n",
      "model_0训练结果更新为第49个模型\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.008274821564555168, Val Loss: 0.008362120017409325\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.008362120017409325, Val Loss: 0.007729974575340748\n",
      "model_0训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.007729974575340748, Val Loss: 0.007630351930856705\n",
      "model_0训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.007630351930856705, Val Loss: 0.0077777341939508915\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.0077777341939508915, Val Loss: 0.007340666372328997\n",
      "model_0训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0073, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Train Loss: 0.007340666372328997, Val Loss: 0.0069429706782102585\n",
      "model_0训练结果更新为第55个模型\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.0069429706782102585, Val Loss: 0.0069868601858615875\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.0069868601858615875, Val Loss: 0.00695425970479846\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.00695425970479846, Val Loss: 0.006609535776078701\n",
      "model_0训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.006609535776078701, Val Loss: 0.00619152095168829\n",
      "model_0训练结果更新为第59个模型\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.00619152095168829, Val Loss: 0.005973190534859896\n",
      "model_0训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.005973190534859896, Val Loss: 0.00595456175506115\n",
      "model_0训练结果更新为第61个模型\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.00595456175506115, Val Loss: 0.00603262847289443\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.00603262847289443, Val Loss: 0.006176680326461792\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.006176680326461792, Val Loss: 0.0064483946189284325\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.0064483946189284325, Val Loss: 0.006887928582727909\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.006887928582727909, Val Loss: 0.00769508071243763\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.00769508071243763, Val Loss: 0.00825160276144743\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.00825160276144743, Val Loss: 0.008211225271224976\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.008211225271224976, Val Loss: 0.006020500790327787\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.006020500790327787, Val Loss: 0.004531688988208771\n",
      "model_0训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.004531688988208771, Val Loss: 0.005795716308057308\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.005795716308057308, Val Loss: 0.005741069093346596\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.005741069093346596, Val Loss: 0.004260961432009935\n",
      "model_0训练结果更新为第73个模型\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.004260961432009935, Val Loss: 0.004782188218086958\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.004782188218086958, Val Loss: 0.005049890838563442\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.005049890838563442, Val Loss: 0.003983368165791035\n",
      "model_0训练结果更新为第76个模型\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.003983368165791035, Val Loss: 0.00438061123713851\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.00438061123713851, Val Loss: 0.004507422912865877\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.004507422912865877, Val Loss: 0.003737984225153923\n",
      "model_0训练结果更新为第79个模型\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.003737984225153923, Val Loss: 0.004119186662137508\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.004119186662137508, Val Loss: 0.0040911715477705\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.0040911715477705, Val Loss: 0.0035559467505663633\n",
      "model_0训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.0035559467505663633, Val Loss: 0.0038615844678133726\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.0038615844678133726, Val Loss: 0.0038005695678293705\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.0038005695678293705, Val Loss: 0.003403257578611374\n",
      "model_0训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.003403257578611374, Val Loss: 0.0035851774737238884\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.0035851774737238884, Val Loss: 0.0036001591943204403\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.0036001591943204403, Val Loss: 0.0032847269903868437\n",
      "model_0训练结果更新为第88个模型\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.0032847269903868437, Val Loss: 0.003283970057964325\n",
      "model_0训练结果更新为第89个模型\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.003283970057964325, Val Loss: 0.0034159792121499777\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.0034159792121499777, Val Loss: 0.003272675909101963\n",
      "model_0训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.003272675909101963, Val Loss: 0.0030629474204033613\n",
      "model_0训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.0030629474204033613, Val Loss: 0.0031094984151422977\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.0031094984151422977, Val Loss: 0.003197460900992155\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.003197460900992155, Val Loss: 0.0030850127805024385\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.0030850127805024385, Val Loss: 0.0029094067867845297\n",
      "model_0训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.0029094067867845297, Val Loss: 0.002871377393603325\n",
      "model_0训练结果更新为第97个模型\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.002871377393603325, Val Loss: 0.002926183631643653\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.002926183631643653, Val Loss: 0.0029125588480383158\n",
      "         \n",
      "第 0 组模型\n",
      "AUC: 0.9997488325364192\n",
      "ACC: 0.9987646695491044\n",
      "F1: 0.998587570621469\n",
      "Recall: 0.998587570621469\n",
      "MCC: 0.9974898757806346\n",
      "    \n",
      "smiles    O=C1Nc2ccc(Br)cc2/C/1=C\\1/Nc2ccccc2/C1=N\\O\n",
      "active                                             1\n",
      "0                                           0.464013\n",
      "1                                                0.0\n",
      "2                                                0.0\n",
      "                             ...                    \n",
      "676.1                                       0.228942\n",
      "677.1                                       0.168414\n",
      "678.1                                       0.606807\n",
      "679.1                                       0.606233\n",
      "680.1                                        0.62705\n",
      "Name: 0, Length: 2775, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2512, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2511523962020874, Val Loss: 0.24618813395500183\n",
      "model_1训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2462, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.24618813395500183, Val Loss: 0.23795613646507263\n",
      "model_1训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2380, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 0.23795613646507263, Val Loss: 0.22753529250621796\n",
      "model_1训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2275, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.22753529250621796, Val Loss: 0.21497437357902527\n",
      "model_1训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2150, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.21497437357902527, Val Loss: 0.1996915191411972\n",
      "model_1训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.1997, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.1996915191411972, Val Loss: 0.18260842561721802\n",
      "model_1训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1826, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.18260842561721802, Val Loss: 0.16323383152484894\n",
      "model_1训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1632, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.16323383152484894, Val Loss: 0.14287272095680237\n",
      "model_1训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1429, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.14287272095680237, Val Loss: 0.12252840399742126\n",
      "model_1训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.1225, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.12252840399742126, Val Loss: 0.10359452664852142\n",
      "model_1训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.1036, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.10359452664852142, Val Loss: 0.08719397336244583\n",
      "model_1训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.0872, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.08719397336244583, Val Loss: 0.07396939396858215\n",
      "model_1训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.0740, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.07396939396858215, Val Loss: 0.06414900720119476\n",
      "model_1训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.0641, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.06414900720119476, Val Loss: 0.05706155672669411\n",
      "model_1训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.0571, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.05706155672669411, Val Loss: 0.052031151950359344\n",
      "model_1训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.0520, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.052031151950359344, Val Loss: 0.04809655249118805\n",
      "model_1训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0481, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.04809655249118805, Val Loss: 0.04472098499536514\n",
      "model_1训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0447, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.04472098499536514, Val Loss: 0.0415969043970108\n",
      "model_1训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0416, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.0415969043970108, Val Loss: 0.03851994499564171\n",
      "model_1训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0385, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.03851994499564171, Val Loss: 0.03556221351027489\n",
      "model_1训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0356, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.03556221351027489, Val Loss: 0.032780375331640244\n",
      "model_1训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0328, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.032780375331640244, Val Loss: 0.03022889979183674\n",
      "model_1训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0302, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.03022889979183674, Val Loss: 0.027881011366844177\n",
      "model_1训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0279, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.027881011366844177, Val Loss: 0.0258201751857996\n",
      "model_1训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0258, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.0258201751857996, Val Loss: 0.024079933762550354\n",
      "model_1训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0241, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.024079933762550354, Val Loss: 0.023089243099093437\n",
      "model_1训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.023089243099093437, Val Loss: 0.02329637110233307\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0233, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.02329637110233307, Val Loss: 0.022779816761612892\n",
      "model_1训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0228, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.022779816761612892, Val Loss: 0.019444145262241364\n",
      "model_1训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0194, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.019444145262241364, Val Loss: 0.019223427399992943\n",
      "model_1训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0192, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.019223427399992943, Val Loss: 0.019742855802178383\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.019742855802178383, Val Loss: 0.017212187871336937\n",
      "model_1训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0172, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.017212187871336937, Val Loss: 0.017438186332583427\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0174, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.017438186332583427, Val Loss: 0.017121795564889908\n",
      "model_1训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.017121795564889908, Val Loss: 0.015299607068300247\n",
      "model_1训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.015299607068300247, Val Loss: 0.01582407020032406\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.01582407020032406, Val Loss: 0.014717334881424904\n",
      "model_1训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.014717334881424904, Val Loss: 0.01383646298199892\n",
      "model_1训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.01383646298199892, Val Loss: 0.014120356179773808\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.014120356179773808, Val Loss: 0.012896907515823841\n",
      "model_1训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.012896907515823841, Val Loss: 0.012455948628485203\n",
      "model_1训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.012455948628485203, Val Loss: 0.012621534988284111\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.012621534988284111, Val Loss: 0.011632531881332397\n",
      "model_1训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.011632531881332397, Val Loss: 0.011055046692490578\n",
      "model_1训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.011055046692490578, Val Loss: 0.011225924827158451\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.011225924827158451, Val Loss: 0.010904312133789062\n",
      "model_1训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.010904312133789062, Val Loss: 0.010232622735202312\n",
      "model_1训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.010232622735202312, Val Loss: 0.009841406717896461\n",
      "model_1训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.009841406717896461, Val Loss: 0.009928216226398945\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.009928216226398945, Val Loss: 0.010116621851921082\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.010116621851921082, Val Loss: 0.009950999170541763\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.009950999170541763, Val Loss: 0.009569190442562103\n",
      "model_1训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.009569190442562103, Val Loss: 0.009040206670761108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.009040206670761108, Val Loss: 0.008672325871884823\n",
      "model_1训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.008672325871884823, Val Loss: 0.008533898741006851\n",
      "model_1训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.008533898741006851, Val Loss: 0.008569006808102131\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.008569006808102131, Val Loss: 0.008746468462049961\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.008746468462049961, Val Loss: 0.009028183296322823\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.009028183296322823, Val Loss: 0.009609385393559933\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.009609385393559933, Val Loss: 0.009907052852213383\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.009907052852213383, Val Loss: 0.009996418841183186\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.009996418841183186, Val Loss: 0.008454829454421997\n",
      "model_1训练结果更新为第61个模型\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.008454829454421997, Val Loss: 0.007248768117278814\n",
      "model_1训练结果更新为第62个模型\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.007248768117278814, Val Loss: 0.007506072986871004\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.007506072986871004, Val Loss: 0.008166934363543987\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.008166934363543987, Val Loss: 0.008170843124389648\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.008170843124389648, Val Loss: 0.0070073106326162815\n",
      "model_1训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.0070073106326162815, Val Loss: 0.0064704339019954205\n",
      "model_1训练结果更新为第67个模型\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.0064704339019954205, Val Loss: 0.006917646620422602\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.006917646620422602, Val Loss: 0.007169654592871666\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.007169654592871666, Val Loss: 0.006907474249601364\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.006907474249601364, Val Loss: 0.00612188084051013\n",
      "model_1训练结果更新为第71个模型\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.00612188084051013, Val Loss: 0.005845705047249794\n",
      "model_1训练结果更新为第72个模型\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.005845705047249794, Val Loss: 0.006139103323221207\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.006139103323221207, Val Loss: 0.006317094899713993\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.006317094899713993, Val Loss: 0.006277116015553474\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.006277116015553474, Val Loss: 0.00577532546594739\n",
      "model_1训练结果更新为第76个模型\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.00577532546594739, Val Loss: 0.005336818750947714\n",
      "model_1训练结果更新为第77个模型\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.005336818750947714, Val Loss: 0.0051162648014724255\n",
      "model_1训练结果更新为第78个模型\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.0051162648014724255, Val Loss: 0.005159767810255289\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.005159767810255289, Val Loss: 0.0053826384246349335\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.0053826384246349335, Val Loss: 0.005667279474437237\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.005667279474437237, Val Loss: 0.006445602979511023\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.006445602979511023, Val Loss: 0.007496431935578585\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.007496431935578585, Val Loss: 0.011246980167925358\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.011246980167925358, Val Loss: 0.013727926649153233\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.013727926649153233, Val Loss: 0.017918238416314125\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0179, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.017918238416314125, Val Loss: 0.005136917345225811\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.005136917345225811, Val Loss: 0.013107489794492722\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.013107489794492722, Val Loss: 0.017135605216026306\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.017135605216026306, Val Loss: 0.005973147228360176\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.005973147228360176, Val Loss: 0.013734898529946804\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.013734898529946804, Val Loss: 0.004660549107939005\n",
      "model_1训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.004660549107939005, Val Loss: 0.010905379429459572\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.010905379429459572, Val Loss: 0.004495515488088131\n",
      "model_1训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.004495515488088131, Val Loss: 0.00931638851761818\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.00931638851761818, Val Loss: 0.004156114999204874\n",
      "model_1训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.004156114999204874, Val Loss: 0.006906387861818075\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.006906387861818075, Val Loss: 0.004580843728035688\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.004580843728035688, Val Loss: 0.004810823127627373\n",
      "         \n",
      "第 1 组模型\n",
      "AUC: 0.9994785157756693\n",
      "ACC: 0.9969116738727609\n",
      "F1: 0.9965635738831615\n",
      "Recall: 1.0\n",
      "MCC: 0.9937787233078857\n",
      "    \n",
      "smiles    Cc1ccc(Sc2cnc(Nc3cccc(Br)n3)s2)cc1C(=O)N(C)CCC#N\n",
      "active                                                   1\n",
      "0                                                 0.324672\n",
      "1                                                      0.0\n",
      "2                                                      0.0\n",
      "                                ...                       \n",
      "676.2                                                  0.0\n",
      "677.2                                                  0.0\n",
      "678.2                                                  0.0\n",
      "679.2                                                  0.0\n",
      "680.2                                             0.384523\n",
      "Name: 0, Length: 2775, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2497, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2497331202030182, Val Loss: 0.2401982545852661\n",
      "model_2训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.2402, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.2401982545852661, Val Loss: 0.22721688449382782\n",
      "model_2训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2272, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.22721688449382782, Val Loss: 0.20706596970558167\n",
      "model_2训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2071, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.20706596970558167, Val Loss: 0.18412943184375763\n",
      "model_2训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.1841, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.18412943184375763, Val Loss: 0.15809135138988495\n",
      "model_2训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.1581, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.15809135138988495, Val Loss: 0.131581112742424\n",
      "model_2训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1316, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.131581112742424, Val Loss: 0.10682635009288788\n",
      "model_2训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1068, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.10682635009288788, Val Loss: 0.08602586388587952\n",
      "model_2训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.0860, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.08602586388587952, Val Loss: 0.07005657255649567\n",
      "model_2训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.0701, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.07005657255649567, Val Loss: 0.0591098815202713\n",
      "model_2训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.0591, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.0591098815202713, Val Loss: 0.05192076787352562\n",
      "model_2训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.0519, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.05192076787352562, Val Loss: 0.04707278311252594\n",
      "model_2训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.0471, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.04707278311252594, Val Loss: 0.04350420832633972\n",
      "model_2训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.0435, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.04350420832633972, Val Loss: 0.04047929495573044\n",
      "model_2训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.0405, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.04047929495573044, Val Loss: 0.037709057331085205\n",
      "model_2训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.0377, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.037709057331085205, Val Loss: 0.0350068062543869\n",
      "model_2训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0350, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.0350068062543869, Val Loss: 0.03239808231592178\n",
      "model_2训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0324, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.03239808231592178, Val Loss: 0.02993128076195717\n",
      "model_2训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0299, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.02993128076195717, Val Loss: 0.027662891894578934\n",
      "model_2训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0277, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.027662891894578934, Val Loss: 0.02563648112118244\n",
      "model_2训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0256, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.02563648112118244, Val Loss: 0.023880083113908768\n",
      "model_2训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0239, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.023880083113908768, Val Loss: 0.02237253449857235\n",
      "model_2训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0224, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.02237253449857235, Val Loss: 0.021087026223540306\n",
      "model_2训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0211, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.021087026223540306, Val Loss: 0.020016159862279892\n",
      "model_2训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.020016159862279892, Val Loss: 0.01913744956254959\n",
      "model_2训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0191, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.01913744956254959, Val Loss: 0.018419554457068443\n",
      "model_2训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0184, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.018419554457068443, Val Loss: 0.017857855185866356\n",
      "model_2训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0179, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.017857855185866356, Val Loss: 0.017373263835906982\n",
      "model_2训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0174, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.017373263835906982, Val Loss: 0.016964297741651535\n",
      "model_2训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0170, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.016964297741651535, Val Loss: 0.016021866351366043\n",
      "model_2训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0160, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.016021866351366043, Val Loss: 0.014991234056651592\n",
      "model_2训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.014991234056651592, Val Loss: 0.014014004729688168\n",
      "model_2训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.014014004729688168, Val Loss: 0.013331416063010693\n",
      "model_2训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.013331416063010693, Val Loss: 0.012975544668734074\n",
      "model_2训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0130, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.012975544668734074, Val Loss: 0.013127366080880165\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.013127366080880165, Val Loss: 0.01471561100333929\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.01471561100333929, Val Loss: 0.013576449826359749\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.013576449826359749, Val Loss: 0.01168818399310112\n",
      "model_2训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0117, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.01168818399310112, Val Loss: 0.01051167119294405\n",
      "model_2训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.01051167119294405, Val Loss: 0.011814425699412823\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0118, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.011814425699412823, Val Loss: 0.011838923208415508\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0118, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.011838923208415508, Val Loss: 0.009286779910326004\n",
      "model_2训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.009286779910326004, Val Loss: 0.0111053716391325\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.0111053716391325, Val Loss: 0.01138706412166357\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0114, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.01138706412166357, Val Loss: 0.008512069471180439\n",
      "model_2训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.008512069471180439, Val Loss: 0.01145793218165636\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.01145793218165636, Val Loss: 0.010591966100037098\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0106, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.010591966100037098, Val Loss: 0.008508515544235706\n",
      "model_2训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.008508515544235706, Val Loss: 0.011476153507828712\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.011476153507828712, Val Loss: 0.008480138145387173\n",
      "model_2训练结果更新为第49个模型\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.008480138145387173, Val Loss: 0.009153478778898716\n",
      "         \n",
      "开始训练第51组epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.009153478778898716, Val Loss: 0.009009166620671749\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.009009166620671749, Val Loss: 0.007255670614540577\n",
      "model_2训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.007255670614540577, Val Loss: 0.008702977560460567\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.008702977560460567, Val Loss: 0.006859674118459225\n",
      "model_2训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.006859674118459225, Val Loss: 0.00815302599221468\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.00815302599221468, Val Loss: 0.0073141190223395824\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.0073141190223395824, Val Loss: 0.007012636866420507\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.007012636866420507, Val Loss: 0.007601980119943619\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.007601980119943619, Val Loss: 0.006278731860220432\n",
      "model_2训练结果更新为第59个模型\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.006278731860220432, Val Loss: 0.007096217013895512\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.007096217013895512, Val Loss: 0.006383766420185566\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.006383766420185566, Val Loss: 0.00619855709373951\n",
      "model_2训练结果更新为第62个模型\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.00619855709373951, Val Loss: 0.006612301804125309\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.006612301804125309, Val Loss: 0.005740789696574211\n",
      "model_2训练结果更新为第64个模型\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.005740789696574211, Val Loss: 0.006099204998463392\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.006099204998463392, Val Loss: 0.006101532839238644\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.006101532839238644, Val Loss: 0.005381695926189423\n",
      "model_2训练结果更新为第67个模型\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.005381695926189423, Val Loss: 0.00575898727402091\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.00575898727402091, Val Loss: 0.005770623218268156\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.005770623218268156, Val Loss: 0.005065439734607935\n",
      "model_2训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.005065439734607935, Val Loss: 0.005226050969213247\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.005226050969213247, Val Loss: 0.00546241132542491\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.00546241132542491, Val Loss: 0.004894615150988102\n",
      "model_2训练结果更新为第73个模型\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.004894615150988102, Val Loss: 0.004633450880646706\n",
      "model_2训练结果更新为第74个模型\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.004633450880646706, Val Loss: 0.0048991297371685505\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.0048991297371685505, Val Loss: 0.0047994800843298435\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.0047994800843298435, Val Loss: 0.004378896206617355\n",
      "model_2训练结果更新为第77个模型\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.004378896206617355, Val Loss: 0.0042113931849598885\n",
      "model_2训练结果更新为第78个模型\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.0042113931849598885, Val Loss: 0.004346553701907396\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.004346553701907396, Val Loss: 0.004347674548625946\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.004347674548625946, Val Loss: 0.004053798504173756\n",
      "model_2训练结果更新为第81个模型\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.004053798504173756, Val Loss: 0.0037722575943917036\n",
      "model_2训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.0037722575943917036, Val Loss: 0.0037005350459367037\n",
      "model_2训练结果更新为第83个模型\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.0037005350459367037, Val Loss: 0.003756921039894223\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.003756921039894223, Val Loss: 0.003785004373639822\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.003785004373639822, Val Loss: 0.00370332645252347\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.00370332645252347, Val Loss: 0.0035696271806955338\n",
      "model_2训练结果更新为第87个模型\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.0035696271806955338, Val Loss: 0.003383859060704708\n",
      "model_2训练结果更新为第88个模型\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.003383859060704708, Val Loss: 0.0032142051495611668\n",
      "model_2训练结果更新为第89个模型\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.0032142051495611668, Val Loss: 0.0030518793500959873\n",
      "model_2训练结果更新为第90个模型\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.0030518793500959873, Val Loss: 0.002925967099145055\n",
      "model_2训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.002925967099145055, Val Loss: 0.0028279356192797422\n",
      "model_2训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.0028279356192797422, Val Loss: 0.0027575590647757053\n",
      "model_2训练结果更新为第93个模型\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.0027575590647757053, Val Loss: 0.0027073905803263187\n",
      "model_2训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.0027073905803263187, Val Loss: 0.0026732704136520624\n",
      "model_2训练结果更新为第95个模型\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.0026732704136520624, Val Loss: 0.002661534119397402\n",
      "model_2训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.002661534119397402, Val Loss: 0.0026828423142433167\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.0026828423142433167, Val Loss: 0.002805945463478565\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.002805945463478565, Val Loss: 0.003171208081766963\n",
      "         \n",
      "第 2 组模型\n",
      "AUC: 0.999471598821804\n",
      "ACC: 0.9993823347745522\n",
      "F1: 0.9993164730006835\n",
      "Recall: 1.0\n",
      "MCC: 0.9987538696190189\n",
      "    \n",
      "smiles    CC(=O)Nc1ccc(cc1)O\n",
      "active                     0\n",
      "0                    0.10272\n",
      "1                        0.0\n",
      "2                   0.007808\n",
      "                 ...        \n",
      "676.2               0.096382\n",
      "677.2                0.07112\n",
      "678.2                    0.0\n",
      "679.2               0.352332\n",
      "680.2               0.428553\n",
      "Name: 0, Length: 2775, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2542, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.25419190526008606, Val Loss: 0.24491538107395172\n",
      "model_3训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.2449, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.24491538107395172, Val Loss: 0.2276630401611328\n",
      "model_3训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2277, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.2276630401611328, Val Loss: 0.20765146613121033\n",
      "model_3训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2077, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.20765146613121033, Val Loss: 0.17970037460327148\n",
      "model_3训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.1797, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.17970037460327148, Val Loss: 0.15035919845104218\n",
      "model_3训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.1504, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.15035919845104218, Val Loss: 0.12066157162189484\n",
      "model_3训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1207, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.12066157162189484, Val Loss: 0.09416734427213669\n",
      "model_3训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.0942, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.09416734427213669, Val Loss: 0.07352372258901596\n",
      "model_3训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.0735, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.07352372258901596, Val Loss: 0.05976032093167305\n",
      "model_3训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.0598, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.05976032093167305, Val Loss: 0.05151497572660446\n",
      "model_3训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.0515, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.05151497572660446, Val Loss: 0.046650439500808716\n",
      "model_3训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.0467, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.046650439500808716, Val Loss: 0.043351735919713974\n",
      "model_3训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.0434, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.043351735919713974, Val Loss: 0.040673598647117615\n",
      "model_3训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.0407, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.040673598647117615, Val Loss: 0.0380396693944931\n",
      "model_3训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.0380, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.0380396693944931, Val Loss: 0.035421743988990784\n",
      "model_3训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.0354, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.035421743988990784, Val Loss: 0.03280938044190407\n",
      "model_3训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0328, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.03280938044190407, Val Loss: 0.03034312278032303\n",
      "model_3训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0303, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.03034312278032303, Val Loss: 0.02809782139956951\n",
      "model_3训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0281, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.02809782139956951, Val Loss: 0.026129603385925293\n",
      "model_3训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0261, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.026129603385925293, Val Loss: 0.024480195716023445\n",
      "model_3训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0245, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.024480195716023445, Val Loss: 0.02306850254535675\n",
      "model_3训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.02306850254535675, Val Loss: 0.021851448342204094\n",
      "model_3训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0219, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.021851448342204094, Val Loss: 0.020812448114156723\n",
      "model_3训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0208, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.020812448114156723, Val Loss: 0.019886748865246773\n",
      "model_3训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.019886748865246773, Val Loss: 0.019021950662136078\n",
      "model_3训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0190, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.019021950662136078, Val Loss: 0.01820635236799717\n",
      "model_3训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0182, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.01820635236799717, Val Loss: 0.017444347962737083\n",
      "model_3训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0174, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.017444347962737083, Val Loss: 0.016732221469283104\n",
      "model_3训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0167, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.016732221469283104, Val Loss: 0.0160861536860466\n",
      "model_3训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0161, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.0160861536860466, Val Loss: 0.01556563563644886\n",
      "model_3训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.01556563563644886, Val Loss: 0.015368444845080376\n",
      "model_3训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.015368444845080376, Val Loss: 0.01552397757768631\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.01552397757768631, Val Loss: 0.015036549419164658\n",
      "model_3训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.015036549419164658, Val Loss: 0.01344621367752552\n",
      "model_3训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.01344621367752552, Val Loss: 0.013211009092628956\n",
      "model_3训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.013211009092628956, Val Loss: 0.013284450396895409\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.013284450396895409, Val Loss: 0.012008866295218468\n",
      "model_3训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0120, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.012008866295218468, Val Loss: 0.011588457971811295\n",
      "model_3训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.011588457971811295, Val Loss: 0.011570119298994541\n",
      "model_3训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.011570119298994541, Val Loss: 0.01047423668205738\n",
      "model_3训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.01047423668205738, Val Loss: 0.009871021844446659\n",
      "model_3训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.009871021844446659, Val Loss: 0.009998493827879429\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.009998493827879429, Val Loss: 0.009825341403484344\n",
      "model_3训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.009825341403484344, Val Loss: 0.009253247641026974\n",
      "model_3训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.009253247641026974, Val Loss: 0.008695580996572971\n",
      "model_3训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.008695580996572971, Val Loss: 0.008564003743231297\n",
      "model_3训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.008564003743231297, Val Loss: 0.008714583702385426\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.008714583702385426, Val Loss: 0.008824200369417667\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.008824200369417667, Val Loss: 0.008577581495046616\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.008577581495046616, Val Loss: 0.007908430881798267\n",
      "model_3训练结果更新为第49个模型\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.007908430881798267, Val Loss: 0.007239875849336386\n",
      "model_3训练结果更新为第50个模型\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.007239875849336386, Val Loss: 0.007054951973259449\n",
      "model_3训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.007054951973259449, Val Loss: 0.007246138993650675\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Train Loss: 0.007246138993650675, Val Loss: 0.007470833603292704\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.007470833603292704, Val Loss: 0.007536097429692745\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.007536097429692745, Val Loss: 0.007188050076365471\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.007188050076365471, Val Loss: 0.006615757010877132\n",
      "model_3训练结果更新为第56个模型\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.006615757010877132, Val Loss: 0.006047572009265423\n",
      "model_3训练结果更新为第57个模型\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.006047572009265423, Val Loss: 0.005751440767198801\n",
      "model_3训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.005751440767198801, Val Loss: 0.005735533311963081\n",
      "model_3训练结果更新为第59个模型\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.005735533311963081, Val Loss: 0.005919445306062698\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.005919445306062698, Val Loss: 0.00631236145272851\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.00631236145272851, Val Loss: 0.00678602047264576\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.00678602047264576, Val Loss: 0.006996441166847944\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.006996441166847944, Val Loss: 0.006296130828559399\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.006296130828559399, Val Loss: 0.004993661306798458\n",
      "model_3训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.004993661306798458, Val Loss: 0.004767467267811298\n",
      "model_3训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.004767467267811298, Val Loss: 0.005482775159180164\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.005482775159180164, Val Loss: 0.005407680757343769\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.005407680757343769, Val Loss: 0.004531343933194876\n",
      "model_3训练结果更新为第69个模型\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.004531343933194876, Val Loss: 0.004300356842577457\n",
      "model_3训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.004300356842577457, Val Loss: 0.004774556029587984\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.004774556029587984, Val Loss: 0.004709306638687849\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.004709306638687849, Val Loss: 0.00407281331717968\n",
      "model_3训练结果更新为第73个模型\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.00407281331717968, Val Loss: 0.0038778847083449364\n",
      "model_3训练结果更新为第74个模型\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.0038778847083449364, Val Loss: 0.004201723728328943\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.004201723728328943, Val Loss: 0.004277743399143219\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.004277743399143219, Val Loss: 0.003892503911629319\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.003892503911629319, Val Loss: 0.0034847103524953127\n",
      "model_3训练结果更新为第78个模型\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.0034847103524953127, Val Loss: 0.003451698226854205\n",
      "model_3训练结果更新为第79个模型\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.003451698226854205, Val Loss: 0.003647161414846778\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.003647161414846778, Val Loss: 0.0037317858077585697\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.0037317858077585697, Val Loss: 0.003593302797526121\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.003593302797526121, Val Loss: 0.0032909889705479145\n",
      "model_3训练结果更新为第83个模型\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.0032909889705479145, Val Loss: 0.0030467223841696978\n",
      "model_3训练结果更新为第84个模型\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.0030467223841696978, Val Loss: 0.0029960134997963905\n",
      "model_3训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.0029960134997963905, Val Loss: 0.0030871194321662188\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.0030871194321662188, Val Loss: 0.003203614382073283\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.003203614382073283, Val Loss: 0.0032594907097518444\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.0032594907097518444, Val Loss: 0.0033030842896550894\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.0033030842896550894, Val Loss: 0.003279756987467408\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.003279756987467408, Val Loss: 0.003275425871834159\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.003275425871834159, Val Loss: 0.0031974990852177143\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.0031974990852177143, Val Loss: 0.0031214719638228416\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.0031214719638228416, Val Loss: 0.002942936960607767\n",
      "model_3训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.002942936960607767, Val Loss: 0.002734065055847168\n",
      "model_3训练结果更新为第95个模型\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.002734065055847168, Val Loss: 0.0025032360572367907\n",
      "model_3训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.0025032360572367907, Val Loss: 0.0023639737628400326\n",
      "model_3训练结果更新为第97个模型\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.0023639737628400326, Val Loss: 0.002355326432734728\n",
      "model_3训练结果更新为第98个模型\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.002355326432734728, Val Loss: 0.0024222317151725292\n",
      "         \n",
      "第 3 组模型\n",
      "AUC: 0.9998625393461815\n",
      "ACC: 0.9993823347745522\n",
      "F1: 0.9993069993069993\n",
      "Recall: 1.0\n",
      "MCC: 0.9987506771287719\n",
      "    \n",
      "smiles    Cc1cc(ccc1)[N+](=O)[O-]\n",
      "active                          0\n",
      "0                        0.098662\n",
      "1                             0.0\n",
      "2                             0.0\n",
      "                   ...           \n",
      "676.2                    0.080012\n",
      "677.2                     0.29828\n",
      "678.2                         0.0\n",
      "679.2                    0.432789\n",
      "680.2                    0.430129\n",
      "Name: 0, Length: 2775, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2529, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2529391944408417, Val Loss: 0.24681013822555542\n",
      "model_4训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2468, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.24681013822555542, Val Loss: 0.23787233233451843\n",
      "model_4训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2379, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.23787233233451843, Val Loss: 0.22346241772174835\n",
      "model_4训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2235, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.22346241772174835, Val Loss: 0.20506609976291656\n",
      "model_4训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2051, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.20506609976291656, Val Loss: 0.18318116664886475\n",
      "model_4训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.1832, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.18318116664886475, Val Loss: 0.1603536307811737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_4训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1604, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.1603536307811737, Val Loss: 0.1355806440114975\n",
      "model_4训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1356, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.1355806440114975, Val Loss: 0.11154976487159729\n",
      "model_4训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1115, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.11154976487159729, Val Loss: 0.09114082157611847\n",
      "model_4训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.0911, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.09114082157611847, Val Loss: 0.07501526921987534\n",
      "model_4训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.0750, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.07501526921987534, Val Loss: 0.06375125050544739\n",
      "model_4训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.0638, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.06375125050544739, Val Loss: 0.05639220029115677\n",
      "model_4训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.0564, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.05639220029115677, Val Loss: 0.05178721621632576\n",
      "model_4训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.0518, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.05178721621632576, Val Loss: 0.04848725348711014\n",
      "model_4训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.0485, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.04848725348711014, Val Loss: 0.045920681208372116\n",
      "model_4训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.0459, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.045920681208372116, Val Loss: 0.0434742271900177\n",
      "model_4训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0435, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.0434742271900177, Val Loss: 0.04111381247639656\n",
      "model_4训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0411, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.04111381247639656, Val Loss: 0.03867620974779129\n",
      "model_4训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0387, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.03867620974779129, Val Loss: 0.0363209992647171\n",
      "model_4训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0363, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.0363209992647171, Val Loss: 0.03398840129375458\n",
      "model_4训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.03398840129375458, Val Loss: 0.0319029837846756\n",
      "model_4训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0319, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.0319029837846756, Val Loss: 0.030007602646946907\n",
      "model_4训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0300, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.030007602646946907, Val Loss: 0.028279919177293777\n",
      "model_4训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0283, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.028279919177293777, Val Loss: 0.02666771039366722\n",
      "model_4训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0267, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.02666771039366722, Val Loss: 0.025072261691093445\n",
      "model_4训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0251, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.025072261691093445, Val Loss: 0.02352897822856903\n",
      "model_4训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0235, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.02352897822856903, Val Loss: 0.02213769219815731\n",
      "model_4训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0221, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.02213769219815731, Val Loss: 0.02105088345706463\n",
      "model_4训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0211, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.02105088345706463, Val Loss: 0.020460931584239006\n",
      "model_4训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0205, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.020460931584239006, Val Loss: 0.02068556658923626\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0207, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.02068556658923626, Val Loss: 0.021469540894031525\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0215, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.021469540894031525, Val Loss: 0.018965382128953934\n",
      "model_4训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0190, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.018965382128953934, Val Loss: 0.01744138076901436\n",
      "model_4训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0174, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.01744138076901436, Val Loss: 0.018031885847449303\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0180, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.018031885847449303, Val Loss: 0.016685619950294495\n",
      "model_4训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0167, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.016685619950294495, Val Loss: 0.015058566816151142\n",
      "model_4训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.015058566816151142, Val Loss: 0.015029127709567547\n",
      "model_4训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.015029127709567547, Val Loss: 0.01456322893500328\n",
      "model_4训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.01456322893500328, Val Loss: 0.013455577194690704\n",
      "model_4训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.013455577194690704, Val Loss: 0.013052990660071373\n",
      "model_4训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.013052990660071373, Val Loss: 0.013058185577392578\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.013058185577392578, Val Loss: 0.012505866587162018\n",
      "model_4训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.012505866587162018, Val Loss: 0.011647718958556652\n",
      "model_4训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.011647718958556652, Val Loss: 0.011531390249729156\n",
      "model_4训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.011531390249729156, Val Loss: 0.011634600348770618\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.011634600348770618, Val Loss: 0.010911575518548489\n",
      "model_4训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.010911575518548489, Val Loss: 0.010158782824873924\n",
      "model_4训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.010158782824873924, Val Loss: 0.009718150831758976\n",
      "model_4训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.009718150831758976, Val Loss: 0.009660963900387287\n",
      "model_4训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.009660963900387287, Val Loss: 0.00992716196924448\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.00992716196924448, Val Loss: 0.010089864954352379\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.010089864954352379, Val Loss: 0.0108179384842515\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0108, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.0108179384842515, Val Loss: 0.009908345527946949\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.009908345527946949, Val Loss: 0.00899519957602024\n",
      "model_4训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.00899519957602024, Val Loss: 0.007908043451607227\n",
      "model_4训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.007908043451607227, Val Loss: 0.007943501695990562\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.007943501695990562, Val Loss: 0.00866214744746685\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.00866214744746685, Val Loss: 0.008540412411093712\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.008540412411093712, Val Loss: 0.008041683584451675\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.008041683584451675, Val Loss: 0.0069824145175516605\n",
      "model_4训练结果更新为第59个模型\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.0069824145175516605, Val Loss: 0.006704424507915974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_4训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.006704424507915974, Val Loss: 0.007142206188291311\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.007142206188291311, Val Loss: 0.007431877311319113\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.007431877311319113, Val Loss: 0.007650543935596943\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.007650543935596943, Val Loss: 0.0067820558324456215\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.0067820558324456215, Val Loss: 0.006040431559085846\n",
      "model_4训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.006040431559085846, Val Loss: 0.005708653945475817\n",
      "model_4训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.005708653945475817, Val Loss: 0.005913927685469389\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.005913927685469389, Val Loss: 0.006340296473354101\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.006340296473354101, Val Loss: 0.006230151280760765\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.006230151280760765, Val Loss: 0.006102325860410929\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.006102325860410929, Val Loss: 0.005416954401880503\n",
      "model_4训练结果更新为第71个模型\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.005416954401880503, Val Loss: 0.004949898924678564\n",
      "model_4训练结果更新为第72个模型\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.004949898924678564, Val Loss: 0.004777212627232075\n",
      "model_4训练结果更新为第73个模型\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.004777212627232075, Val Loss: 0.004882017150521278\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.004882017150521278, Val Loss: 0.005223357118666172\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.005223357118666172, Val Loss: 0.005564636085182428\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.005564636085182428, Val Loss: 0.0067311841994524\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.0067311841994524, Val Loss: 0.007370332721620798\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.007370332721620798, Val Loss: 0.009954677894711494\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.009954677894711494, Val Loss: 0.007228550501167774\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.007228550501167774, Val Loss: 0.004620022606104612\n",
      "model_4训练结果更新为第81个模型\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.004620022606104612, Val Loss: 0.0044648731127381325\n",
      "model_4训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.0044648731127381325, Val Loss: 0.005845104809850454\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.005845104809850454, Val Loss: 0.0049812328070402145\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.0049812328070402145, Val Loss: 0.0038289022631943226\n",
      "model_4训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.0038289022631943226, Val Loss: 0.005430641584098339\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.005430641584098339, Val Loss: 0.004859425593167543\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.004859425593167543, Val Loss: 0.003550370456650853\n",
      "model_4训练结果更新为第88个模型\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.003550370456650853, Val Loss: 0.005230990704149008\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.005230990704149008, Val Loss: 0.004613307770341635\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.004613307770341635, Val Loss: 0.0033505188766866922\n",
      "model_4训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.0033505188766866922, Val Loss: 0.00499214930459857\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.00499214930459857, Val Loss: 0.004228329751640558\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.004228329751640558, Val Loss: 0.003230143105611205\n",
      "model_4训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.003230143105611205, Val Loss: 0.004620058927685022\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.004620058927685022, Val Loss: 0.003605858888477087\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.003605858888477087, Val Loss: 0.0032488659489899874\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.0032488659489899874, Val Loss: 0.00409554922953248\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.00409554922953248, Val Loss: 0.002947369357571006\n",
      "model_4训练结果更新为第99个模型\n",
      "         \n",
      "第 4 组模型\n",
      "AUC: 0.999498284256036\n",
      "ACC: 0.9987646695491044\n",
      "F1: 0.9985955056179775\n",
      "Recall: 0.9985955056179775\n",
      "MCC: 0.997492969785563\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "for i in range(start_index, end_index+1):\n",
    "    \n",
    "    # 读取数据\n",
    "    file_path = file_prefix + str(i) + file_extension\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    first_row = data.iloc[0]\n",
    "    print(first_row)\n",
    "    \n",
    "    # 提取特征和标签\n",
    "    x_input = data.drop(['smiles'], axis=1).values\n",
    "    y_output = data['active'].values\n",
    "    \n",
    "        # 遍历数据，将 inf 赋值为 0\n",
    "    x_input = np.nan_to_num(x_input, posinf=0, neginf=0)\n",
    "    \n",
    "#        # 检查并输出非 float64 数据的坐标\n",
    "#    for row_index, row in enumerate(x_input):\n",
    "#        for col_index, value in enumerate(row):\n",
    "#            if x_input.dtype!= np.float64:\n",
    "#                print(f\"非 float64 数据位于 ({row_index}, {col_index})，值为: {value}\")\n",
    "#    \n",
    "    #print('x_input',x_input)\n",
    "    #print('y_output',y_output)\n",
    "    \n",
    "    \n",
    "    val_x_input = data.drop(['smiles'], axis=1).values\n",
    "    val_y_output = data['active'].values\n",
    "    \n",
    "    val_x_input = np.nan_to_num(x_input, posinf=0, neginf=0)\n",
    "    \n",
    "#    # 查找 x_input 中的无穷值位置\n",
    "#    infinity_positions_x = np.argwhere(np.isinf(x_input))\n",
    "#    if len(infinity_positions_x) > 0:\n",
    "#        column_names = data.drop(['SMILES'], axis=1).columns\n",
    "#        infinity_column_names_x = [column_names[pos[1]] for pos in infinity_positions_x]\n",
    "#        print(\"在 x_input 中的无穷值所在列名：\", infinity_column_names_x, '         位置：',infinity_positions_x)\n",
    "#    else:\n",
    "#        print(\"在 x_input 中没有无穷值\")\n",
    "#\n",
    "#    # 查找 val_x_input 中的无穷值位置\n",
    "#    infinity_positions_val_x = np.argwhere(np.isinf(val_x_input))\n",
    "#    if len(infinity_positions_val_x) > 0:\n",
    "#        column_names = data.drop(['SMILES'], axis=1).columns\n",
    "#        infinity_column_names_val_x = [column_names[pos[1]] for pos in infinity_positions_val_x]\n",
    "#        print(\"在 val_x_input 中的无穷值所在列名：\", infinity_column_names_val_x, '        位置：',infinity_positions_x)\n",
    "#    else:\n",
    "#        print(\"在 val_x_input 中没有无穷值\")\n",
    "    \n",
    "        # 标准化特征（假设特征是数值型的）\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    x_input = scaler.fit_transform(x_input)\n",
    "    val_x_input = scaler.fit_transform(val_x_input)\n",
    "\n",
    "    # 数据放缩处理（例如，将特征值缩放到 0 到 1 之间）\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_input = min_max_scaler.fit_transform(x_input)\n",
    "    val_x_input = min_max_scaler.fit_transform(val_x_input)\n",
    "    \n",
    "        # 转换为Tensor\n",
    "    x = torch.Tensor(x_input)\n",
    "    y_true = torch.Tensor(y_output).view(-1, 1)    \n",
    "    \n",
    "    val_x = torch.Tensor(val_x_input)\n",
    "    val_y_true = torch.Tensor(val_y_output).view(-1, 1)\n",
    "    \n",
    "    \n",
    "    # 数据维度\n",
    "    input_dim = x.shape[1]\n",
    "    output_dim = 1\n",
    "    hidden_dim1 = 256\n",
    "    hidden_dim2 = 128\n",
    "    hidden_dim3 = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    weight_decay = 0.001\n",
    "    # MLP模型定义\n",
    "    mlp = nn.Sequential(  \n",
    "        nn.Linear(input_dim, hidden_dim1),  # 第一层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim1, hidden_dim2),  # 第二层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim2, hidden_dim3),  # 第三层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim3, output_dim),  # 输出层  \n",
    "        nn.Sigmoid()  \n",
    "    )  \n",
    "        \n",
    "    # 优化器和损失函数\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "    #loss_func =  nn.BCEWithLogitsLoss()\n",
    "    loss_func =  nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    num_iterations_without_improvement = 0  # Initialize counter for iterations without improvement. \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"开始训练第{epoch}组epochs\")\n",
    "        prediction = mlp(x)\n",
    "        #print('prediction',prediction)\n",
    "        \n",
    "        #prediction_np = prediction.detach().numpy()\n",
    "        #df = pd.DataFrame(prediction_np)\n",
    "        #df.to_csv('prediction.csv', index=False)\n",
    "        \n",
    "        loss = loss_func(prediction, y_true)\n",
    "        print('loss',loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(optimizer)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_prediction=mlp(val_x)\n",
    "            val_loss = loss_func(val_prediction, val_y_true)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "        \n",
    "        #print(\"检查数据和模型输出:\")\n",
    "        #print(\"是否存在 NaN 或无穷大值在训练预测结果中:\", torch.isnan(prediction).any() or torch.isinf(prediction).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在验证预测结果中:\", torch.isnan(val_prediction).any() or torch.isinf(val_prediction).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在训练真实值中:\", torch.isnan(y_true).any() or torch.isinf(y_true).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在验证真实值中:\", torch.isnan(val_y_true).any() or torch.isinf(val_y_true).any())\n",
    "        #\n",
    "          \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  \n",
    "            num_iterations_without_improvement = 0\n",
    "            torch.save(mlp.state_dict(), f\"models/models_{file}/model_{i+1}.pt\")\n",
    "            #torch.save(mlp, \"model_saved/mlp1.pt\")\n",
    "            print(f'model_{i}训练结果更新为第{epoch}个模型')\n",
    "            best_epoch = epoch\n",
    "        else :\n",
    "            num_iterations_without_improvement += 1  \n",
    "            if num_iterations_without_improvement == 100:\n",
    "                print('100次迭代没有更新，结束迭代')\n",
    "                print(f'最棒的模型是第{best_epoch}个epoch')\n",
    "                break\n",
    "        print('         ')\n",
    "        \n",
    "\n",
    "    prediction_np = prediction.data.numpy()\n",
    "        # 将 prediction 添加到 DataFrame\n",
    "    #data['prediction'] = prediction_np\n",
    "        # 将 DataFrame 保存到 CSV 文件\n",
    "    #data.to_csv('output/output_train.csv', index=True)\n",
    "    \n",
    "    # 预测并计算AUC\n",
    "    prediction_np = prediction.detach().numpy().flatten()\n",
    "    auc = roc_auc_score(y_true, prediction_np)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    binary_prediction = np.where(prediction_np > threshold, 1, 0)\n",
    "    \n",
    "    acc = accuracy_score(y_true, binary_prediction)\n",
    "    f1 = f1_score(y_true, binary_prediction)\n",
    "    recall = recall_score(y_true, binary_prediction)\n",
    "    mcc = matthews_corrcoef(y_true, binary_prediction)\n",
    "    output_file = \"train_scores.csv\"\n",
    "    \n",
    "    \n",
    "    auc_scores.append(auc)\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    recall_scores.append(recall)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    print('第',i,'组模型')\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"ACC:\", acc)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print('    ')\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972b13e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_1: 0.9739350320949232\n",
      "Accuracy_1: 0.9359605911330049\n",
      "f1_1: 0.9319371727748691\n",
      "recall_1: 0.9175257731958762\n",
      "mcc_1: 0.8718836478204887\n",
      "             \n",
      "             \n",
      "AUC_2: 0.9716999608303956\n",
      "Accuracy_2: 0.916256157635468\n",
      "f1_2: 0.9137055837563451\n",
      "recall_2: 0.9782608695652174\n",
      "mcc_2: 0.8399241354423864\n",
      "             \n",
      "             \n",
      "AUC_3: 0.950197628458498\n",
      "Accuracy_3: 0.8866995073891626\n",
      "f1_3: 0.8756756756756756\n",
      "recall_3: 0.9204545454545454\n",
      "mcc_3: 0.7751445230514818\n",
      "             \n",
      "             \n",
      "AUC_4: 0.9636363636363636\n",
      "Accuracy_4: 0.8669950738916257\n",
      "f1_4: 0.8615384615384616\n",
      "recall_4: 0.9032258064516129\n",
      "mcc_4: 0.7370004403209095\n",
      "             \n",
      "             \n",
      "Average AUC: 0.964867±0.009295\n",
      "Average Accuracy: 0.901478±0.026528\n",
      "Average F1: 0.895714±0.028308\n",
      "Average Recall: 0.929867±0.028691\n",
      "Average MCC: 0.805988±0.052926\n"
     ]
    }
   ],
   "source": [
    "te_file_prefix = f\"{file}_Feature_fusion/data_test\"\n",
    "te_file_extension = \".csv\"\n",
    "\n",
    "import os \n",
    "\n",
    "avg_auc = 0.0  \n",
    "avg_acc = 0.0  \n",
    "avg_f1 = 0.0  \n",
    "avg_recall = 0.0  \n",
    "avg_mcc = 0.0  \n",
    "\n",
    "\n",
    "#te_file_paths = [te_file_prefix + str(i) + te_file_extension for i in range(start_index, end_index+1)]  \n",
    "results = pd.DataFrame(columns=['AUC', 'Accuracy', 'F1', 'Recall', 'MCC'])   \n",
    "    \n",
    "#for i,te_file_path in enumerate(te_file_paths):  \n",
    "for i in range(start_index, end_index): \n",
    "    te_file_path = te_file_prefix + str(i) + te_file_extension\n",
    "    data_test = pd.read_csv(te_file_path)  \n",
    "    if file == 'BBB':\n",
    "        data_test = data_test.head(129)  \n",
    "\n",
    "    #print(data_test)\n",
    "      \n",
    "    te_x_input = data_test.drop(['smiles'], axis=1).values  \n",
    "    #print('1te_x_input.shape',te_x_input.shape)\n",
    "    \n",
    "    te_y_true = data_test['active'].values  \n",
    "    te_x_input = te_x_input.astype(np.float64)\n",
    "    #print('2te_x_input.shape',te_x_input.shape)\n",
    "    \n",
    "    te_x_input = np.nan_to_num(te_x_input, posinf=0, neginf=0)\n",
    "    #print('3te_x_input',te_x_input.shape)    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    te_x_input = scaler.fit_transform(te_x_input)\n",
    "\n",
    "    # 数据放缩处理（例如，将特征值缩放到 0 到 1 之间）\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    te_x_input = min_max_scaler.fit_transform(te_x_input)\n",
    "    te_x_input = torch.Tensor(te_x_input)\n",
    "    #print('4te_x_input.shape',te_x_input.shape)\n",
    "    mlp.load_state_dict(torch.load(f\"models/models_{file}/model_{i+1}.pt\")) \n",
    "    \n",
    "    #print('39 te_x_input te_x_input ',te_x_input)\n",
    "    \n",
    "    #print(\"Length of te_prediction:\", len(te_prediction))\n",
    "    #print(\"Length of data_test:\", len(data_test))\n",
    "    #print(\"Length of data index:\", len(data.index))\n",
    "    #\n",
    "    with torch.no_grad():    \n",
    "        \n",
    "       # print(\"47  te_x_input  te_x_input\",te_x_input)\n",
    "        \n",
    "        te_prediction = mlp(te_x_input)  \n",
    "        \n",
    "        ############################################################\n",
    "        te_prediction = te_prediction.numpy()\n",
    "        data = pd.DataFrame(te_prediction)\n",
    "        data.to_csv(f'output1/test_prediction{i+1}.csv', mode='a', index=False)\n",
    "        ############################################################\n",
    "        \n",
    "        #print('te_y_true',len(te_y_true))\n",
    "        #print('te_prediction',len(te_prediction))\n",
    "        #print('te_y_true',te_y_true)\n",
    "        #print('te_prediction',te_prediction)\n",
    "        #print('检查 te_y_true 是否包含 NaN',np.isnan(te_y_true).any())  # 检查 te_y_true 是否包含 NaN\n",
    "        #print('检查 te_y_true 是否包含无穷值',np.isinf(te_y_true).any())  # 检查 te_y_true 是否包含无穷值\n",
    "        #print('检查 te_prediction 是否包含 NaN',np.isnan(te_prediction).any())  # 检查 te_prediction 是否包含 NaN\n",
    "        #print('检查 te_prediction 是否包含无穷值',np.isinf(te_prediction).any())  # 检查 te_prediction 是否包含无穷值\n",
    "        #\n",
    "        auc = roc_auc_score(te_y_true, te_prediction)\n",
    "        #print(auc)\n",
    "        \n",
    "        #print('54  te_predictionte_prediction',te_prediction)    \n",
    "        #te_prediction = te_prediction.numpy()      \n",
    "        data_test[f'te_prediction_{i+1}'] = te_prediction \n",
    "        \n",
    "        auc = roc_auc_score(te_y_true, te_prediction)\n",
    "        #print(auc)\n",
    "        \n",
    "        #data_test.to_csv(f'output1/te_prediction{i+1}.csv', mode='a', index=True)\n",
    "        #print('te_prediction  ',te_prediction)\n",
    "        \n",
    "        auc = roc_auc_score(te_y_true, te_prediction)  \n",
    "        print(f'AUC_{i+1}: {auc}')\n",
    "        \n",
    "        te_prediction_binary = np.where(te_prediction > 0.5, 1, 0)  \n",
    "        acc = accuracy_score(te_y_true, te_prediction_binary)  \n",
    "        print(f'Accuracy_{i+1}: {acc}')\n",
    "        f1 = f1_score(te_y_true, te_prediction_binary)\n",
    "        recall = recall_score(te_y_true, te_prediction_binary)\n",
    "        mcc = matthews_corrcoef(te_y_true, te_prediction_binary)\n",
    "        print(f'f1_{i+1}: {f1}')\n",
    "        print(f'recall_{i+1}: {recall}')\n",
    "        print(f'mcc_{i+1}: {mcc}')\n",
    "        print('             ')\n",
    "        print('             ')\n",
    "        \n",
    "        avg_auc += auc  \n",
    "        avg_acc += acc  \n",
    "        avg_f1 += f1  \n",
    "        avg_recall += recall  \n",
    "        avg_mcc += mcc\n",
    "        \n",
    "        results = results.append({'AUC': auc, 'Accuracy': acc, 'F1': f1, 'Recall': recall, 'MCC': mcc}, ignore_index=True) \n",
    "results.to_csv('测试集预测_results.csv', index=False)\n",
    "\n",
    "avg_auc /= len(range(start_index, end_index)) \n",
    "avg_acc /= len(range(start_index, end_index)) \n",
    "avg_f1 /= len(range(start_index, end_index))  \n",
    "avg_recall /= len(range(start_index, end_index))\n",
    "avg_mcc /= len(range(start_index, end_index))\n",
    "\n",
    "auc_te_values = results['AUC'].values\n",
    "accuracy_te_values = results['Accuracy'].values\n",
    "f1_te_values = results['F1'].values\n",
    "recall_te_values = results['Recall'].values\n",
    "mcc_te_values = results['MCC'].values\n",
    "\n",
    "std_auc_te = np.std(auc_te_values)\n",
    "std_acc_te = np.std(accuracy_te_values)\n",
    "std_f1_te = np.std(f1_te_values)\n",
    "std_recall_te = np.std(recall_te_values)\n",
    "std_mcc_te = np.std(mcc_te_values)\n",
    "\n",
    "print(f'Average AUC: {avg_auc:.6f}±{std_auc_te:.6f}')\n",
    "print(f'Average Accuracy: {avg_acc:.6f}±{std_acc_te:.6f}')\n",
    "print(f'Average F1: {avg_f1:.6f}±{std_f1_te:.6f}')\n",
    "print(f'Average Recall: {avg_recall:.6f}±{std_recall_te:.6f}')\n",
    "print(f'Average MCC: {avg_mcc:.6f}±{std_mcc_te:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c28f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aba305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f984609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.8cu111",
   "language": "python",
   "name": "pytorch1.8cu111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
